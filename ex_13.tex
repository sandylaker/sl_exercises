%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
\usepackage{multicol}
%\usepackage{tabularx}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\input{latex-math/ml-feature-sel.tex}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\xone}{x_1}
\newcommand{\xtwo}{x_2}
\newcommand{\txone}{\tilde{x}_1}
\newcommand{\txtwo}{\tilde{x}_2}
\newcommand{\BIC}[1]{\mathrm{BIC}_{\{#1\}}}

\title[]{\textbf{Exercise of Supervised Learning:\\Feature Selection}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{January 19, 2024}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}


\begin{frame}{Exercise 1: Filter Problems}
	Let $f(x_1, x_2| \bm{\mu})$ be the density function of the bivariate Normal distribution with mean $\bm{\mu}$ and covariance $\bm{\Sigma} = \id_2$. You are given the following data generation process (DGP):
	\begin{itemize}
		\item the target $Y \sim \mathrm{Bernoulli}(0.5)$,
		\item the conditional density $p(x_1, x_2 | Y=1) = 0.5 (f(x_1, x_2 | (1, -1 )^T) + f(x_1, x_2 |(-1, 1)^T))$,
		\item the conditional density $p(x_1, x_2 | Y=0) = 0.5 (f(x_1, x_2 | (1, 1)^T) + f(x_1, x_2 | (-1, -1)^T))$.
	\end{itemize}
	(Write the formulas on white board)
	
	(a) Sketch the DGP.
	
	\textbf{Solution: Show the standard solution.}
\end{frame}

\begin{frame}{Exercise 1 (b)}
	(b) Compute $\prob{Y=1 | \xone = \txone}$ and $\prob{Y=1 | \xtwo = \txtwo}$.
	\vspace{5pt}
	
	Hint: $\xone, \xtwo$ are generated based on $Y$.
	
	\begin{align*}
		\prob{Y=1 | \xone = \txone} &= \frac{p(\xone = \txone | Y=1) \prob{Y=1}}{p(\xone = \txone | Y=1) \prob{Y=1} + p(\xone = \txone | Y = 0) \prob{Y=0}} \\
		&= \frac{p(\xone = \txone | Y=1)}{p(\xone = \txone | Y=1) + p(\xone = \txone | Y = 0) } \qquad (\prob{Y=1} = \prob{Y=0})
	\end{align*}
	
	Question: How to get $p(\xone = \txone | Y=1)$ and other terms?
	
\end{frame}

\begin{frame}{Exercise 1 (b): Continued}
	\begin{itemize}
		\item<1-> Marginal over $x_2$ $\leadsto$ $p(\xone = \txone | Y=1) = \int p(\xone = \txone, \xtwo = z| Y=1) \mathrm{d}z$
		\item<2-> Hard to \textbf{directly} marginalize because $(\xone, \xtwo) | Y$ is a mixture of Gaussian components: $p(x_1, x_2 | Y=1) = \mathbf{0.5} (f(x_1, x_2 | (1, -1 )^T) + f(x_1, x_2 |(-1, 1)^T))$
		\item<3-> But it is easy to compute the marginal distribution for a \textbf{single Gaussian}.
		\item<4-> Can we first marginalize individual Gaussian components and then mix up them? Yes.
		\item<5-> Marginalize $x_1, x_2\sim \mathcal{N}((1, -1 )^T, \id_2)$ over $x_2$ $\leadsto$ $x_1 \sim \mathcal{N}(1, 1)$. 
		\item<6-> Marginalize $x_1, x_2  \sim \mathcal{N}((-1, 1 )^T, \id_2)$ over $x_2$ $\leadsto$ $x_1 \sim \mathcal{N}(-1, 1)$. 
		\item<7-> Let $g_\mu: \mathbb{R} \rightarrow [0, 1]$ be the prob. density function of $\mathcal{N}(\mu, 1)$.
		\item<8-> $p(\xone = \txone | Y=1) = \mathbf{0.5} (g_{1}(\txone) + g_{-1}(\txone))$ \qquad (Don't forget the weights of each Gaussian component).
		\item<9-> Similarily: $p(\xone = \txone | Y=0) = \mathbf{0.5} (g_{1}(\txone) + g_{-1}(\txone))$.
		\item<10-> So $\prob{Y=1 | \xone = \txone} = \frac{ \mathbf{0.5} (g_{1}(\txone) + g_{-1}(\txone))}{ \mathbf{0.5} (g_{1}(\txone) + g_{-1}(\txone)) +  \mathbf{0.5} (g_{1}(\txone) + g_{-1}(\txone))} = 0.5$ \quad (Same for $\prob{Y=0 | \xone = \txone}$.
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (c)}
	(c) Compute $\prob{Y=1 | \xone = 1, \xtwo = 1}$.
	\small 
	\begin{align*}
		\prob{Y=1 | \xone = 1, \xtwo = 1} &= \frac{p((1, 1) | Y=1) \prob{Y=1}}{p((1, 1) | Y=1) \prob{Y=1} + p((1, 1) | Y=0) \prob{Y=0}} \\
		&= \frac{1}{1 + \frac{p((1, 1) | Y=1)}{p((1, 1) | Y=1)}} \\
		&= \frac{1}{1 + \frac{\exp(0) + \exp(-0.5 (-2, -2)^T (-2, -2))}{2 \exp(-0.5 (0, -2)^T (0, -2))}} \qquad \text{(Use the given density functions)}\\
		&\approx 0.21
	\end{align*}
\end{frame}

\begin{frame}{Exercise 1 (d)}
	(d) Explain what happens if we apply mutual information as filter in this scenario.
	\begin{itemize}
		\item<2-> From (b): $\prob{Y = 1} = \prob{Y = 1 | \xone = \txone} = \prob{Y = 1 | \xtwo = \txtwo} = 0.5$.
		\item<3-> So $\xone$ is independent from $Y$, and the same hold for $\xtwo$.
		\item<4-> Mutual information between $x_i$ and $Y$ will be $0$ for $i = 1, 2$.
		\item<5-> Any feature will be more preferred over them.
		\item<6-> But $Y$ is clearly jointly dependent on $x_1$ and $x_2$, as shown in (c), $\prob{Y=1 | \xone = 1, \xtwo = 1} \neq \prob{Y=1}$.
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 2: Filter simulation study}
	\textbf{Show the standard solution.}
\end{frame}

\begin{frame}{Exercise 3: Wrappers}
	You are given the following features and their respective BICs. BIC$_i$ with $i \in \{ \{A\}, \{B\}, \{C\}, \{D\}, \{A, B\}, \{A, C \} \{A, D \}, \{B, C \}, \{B, D\}, \{C, D\}, $ $\{A, B, C\}, \{A, B, D\}, \{B, C, D\}, \{A, B, C, D\}\}.$
	\begin{columns}
		\begin{column}{0.3\textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline
					$\{A\}$ & 0.9 \\
					$\{B\}$ & 0.8 \\
					$\{C\}$ & 1.0 \\
					$\{D\}$ & 1.0 \\
					$\{A, B\}$ & 0.8 \\
					$\{A, C\}$ & 0.7 \\
					$\{A, D\}$ & 0.8 \\
				\end{tabular}
			\end{table}
		\end{column}
		\begin{column}{0.3 \textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline
					$\{B, C\}$ & 0.7 \\
					$\{B, D\}$ & 0.6 \\
					$\{C, D\}$ & 0.9 \\
					$\{A, B, C\}$ & 0.6 \\
					$\{A, B, D\}$ & 0.8 \\
					$\{B, C, D\}$ & 0.5 \\
					$\{A, B, C, D\}$ & 0.6 \\
				\end{tabular}
			\end{table}	
		\end{column}
		\hfill
	\end{columns}
	(a) Do forward search and note down each iteration.
	
	(b) Do backward search and note down each iteration.
\end{frame}

\begin{frame}{Exercise 3 (a)}
	\begin{columns}
		\begin{column}{0.3\textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline
					$\{A\}$ & 0.9 \\
					$\{B\}$ & 0.8 \\
					$\{C\}$ & 1.0 \\
					$\{D\}$ & 1.0 \\
					$\{A, B\}$ & 0.8 \\
					$\{A, C\}$ & 0.7 \\
					$\{A, D\}$ & 0.8 \\
				\end{tabular}
			\end{table}
		\end{column}
		\begin{column}{0.3 \textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline					
					$\{B, C\}$ & 0.7 \\
					$\{B, D\}$ & 0.6 \\
					$\{C, D\}$ & 0.9 \\
					$\{A, B, C\}$ & 0.6 \\
					$\{A, B, D\}$ & 0.8 \\
					$\{B, C, D\}$ & 0.5 \\
					$\{A, B, C, D\}$ & 0.6 \\
				\end{tabular}
			\end{table}	
		\end{column}
		\hfill
	\end{columns}
	(a) Do forward search and note down each iteration.
	\begin{enumerate}
		\item<2-> $\{B\}$ since $\BIC{B} < \BIC{X} \qquad \forall X \in \{\{A\}, \{C\}, \{D\} \}$.
		\item<3-> $\{B, D \}$ since $\BIC{B, D} < \BIC{X} \qquad \forall X \in \{\{A, B\}, \{B, C\}\}$
		\item<4-> $\{B, C, D \}$ since $\BIC{B, C, D} < \BIC{A, B, D}$.
		\item<5-> $\{B, C, D \}$ and terminate since $\BIC{B, C, D} < \BIC{A, C, B, D}$.
	\end{enumerate}
\end{frame}

\begin{frame}{Exercise 3 (b)}
	\begin{columns}
		\begin{column}{0.3\textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline
					$\{A\}$ & 0.9 \\
					$\{B\}$ & 0.8 \\
					$\{C\}$ & 1.0 \\
					$\{D\}$ & 1.0 \\
					$\{A, B\}$ & 0.8 \\
					$\{A, C\}$ & 0.7 \\
					$\{A, D\}$ & 0.8 \\
				\end{tabular}
			\end{table}
		\end{column}
		\begin{column}{0.3 \textwidth}
			\centering
			\begin{table}
				\centering
				\begin{tabular}{c c}
					Features & BIC$_i$ \\
					\hline					
					$\{B, C\}$ & 0.7 \\
					$\{B, D\}$ & 0.6 \\
					$\{C, D\}$ & 0.9 \\
					$\{A, B, C\}$ & 0.6 \\
					$\{A, B, D\}$ & 0.8 \\
					$\{B, C, D\}$ & 0.5 \\
					$\{A, B, C, D\}$ & 0.6 \\
				\end{tabular}
			\end{table}	
		\end{column}
		\hfill
	\end{columns}
	(b) Do backward search and note down each iteration.
	\begin{enumerate}
		\item<2-> Start with all features $\{A, B, C, D\}$.
		\item<3-> $\{B, C, D \}$ and since $\BIC{B, C, D} < \BIC{X} \qquad \forall X \in \{\{A, B, C\}, \{A, C, D\} \}$.
		\item<4-> $\{B, C, D \}$ and terminate since $\BIC{B, C, D} < \BIC{X} \qquad \forall X \in \{\{B, C\}, \{B, D\}, \{C, D\} \}$.
	\end{enumerate}
\end{frame}

\end{document}
