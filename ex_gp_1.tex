%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\input{latex-math/ml-gp.tex}

\newcommand{\Norm}{\mathcal{N}}
\newcommand{\sumip}{\sum_i^p}
\newcommand{\thetai}{\thetav_i}
\newcommand{\xvp}{\xv^\prime}
\newcommand{\kz}{k_0}
\newcommand{\ko}{k_1}
\newcommand{\kzxxp}{\kz(\xv, \xvp)}
\newcommand{\koxxp}{\ko(\xv, \xvp)}

\title[]{\textbf{Exercise of Supervised Learning:\\Gaussian Processes 1}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{January 21, 2025}


% This is only inserted into the PDF information catalog. Can be left
% out.
%\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}

\begin{frame}{Exercise 1: Bayesian Linear Model}
\small
In the Bayesian linear model, we assume that the data follows the following law:
\begin{equation*}
	y = \fx + \eps = \thetav^\top \xv + \eps,
\end{equation*}
where $\eps \sim \Norm(0, \sigma^2)$ and independent of $\xv$. On the data-level this corresponds to 
\begin{equation*}
	\yi = \fxi + \epsi = \thetav^\top \xi + \epsi, \quad \text{for } i \in [n],
\end{equation*}
where $\epsi \in \Norm(0, \sigma^2)$ are i.i.d. and all independent of $\xi$'s. In the Bayesian perspective it is assumed that the parameter vector $\thetav$ is stochastic and follows a distribution.
%
Assume we are interested in the so-called maximum a posteriori estimate of $\thetav$, which is defined by $$\thetavh = \argmax_{\thetav} p(\thetav | \Xmat, \yv).$$
%
(a) Show that if we choose a \textbf{uniform distribution} over the parameter vector $\thetav$ as the prior belief, i.e., $q(\thetav) \propto 1$, then the maximum a posteriori estimate coincides with the \textbf{empirical risk minimizer for the L2-loss} (over linear models). 	
\end{frame}

\begin{frame}{Exercise 1(b)}
	Show that if we choose a \textbf{Gaussian distribution} over the parameter vectors $\thetav$ as the prior belief, i.e., 
	\begin{equation*}
		q(\thetav) \propto \exp \left[ - \frac{1}{2 \tau^2} \thetav^\top \thetav \right], \quad \tau > 0,
	\end{equation*}
	then the maximum a posteriori eestimate coincides for a specific choice of $\tau$ with the \textbf{regularized} empirical risk miminizer for the L2-loss with L2 penalty (over the linear models), i.e., the Ridge regression.
\end{frame}

\begin{frame}{Exercise 1(c)}
	Show that if we choose a \textbf{Laplace distribution} over the parameter vectors $\thetav$ as the prior belief, i.e., 
	\begin{equation*}
		q(\thetav) \propto \exp \left[ - \frac{\sumip |\thetai|}{\tau} \right], \quad \tau > 0,
	\end{equation*}
	then the maximum a posteriori estimate coincides for a specific choice of $\tau$ with the regularized empirical risk minimizer for the L2-loss with L1 penalty (over the linear models), i.e., the Lasso regression.
\end{frame}

\begin{frame}{Exercise 2: Covariance Functions}
	\small
	Consider the commonly used covariance functions mentioned in the lecture slides: constant, linear, polynomial, squared exponential, Matern, exponential covariance functions.
	\vspace{1em}

	(a) Show that they are valid covariance functions. (\textbf{Proofs for Matern and exp. cov. functions are out of scope and omitted.}) You may use the following composition rules. In these rules we assume that $k_0(\cdot, \cdot)$ and $k_1(\cdot, \cdot)$ are valid covariance functions.
	\begin{enumerate}
		\item $\kxxp = \xv^\top \xvp$ is a valid covariance function;
		\item $\kxxp = c \cdot \kzxxp$ is a valid covariance function if $c \geq 0$ is constant.
		\item $\kxxp = \kzxxp + \koxxp$ is a valid covariance function;
		\item $\kxxp = \kzxxp \cdot \koxxp$ is a valid covariance function;
		\item $\kxxp = g(\kzxxp)$ is a valid cov. func. if $g$ is a polynomial function with \textbf{pos.} coefficients;
		\item $\kxxp = t(\xv) \cdot \kzxxp \cdot t(\xvp)$ is a valid covariance function, where $t$ is any function;
		\item $\kxxp = \exp(\koxxp)$ is a valid covariance function;
		\item $\kxxp = \xv^\top \Amat \xvp$ is a valid covariance function if $\Amat \succeq 0$.
	\end{enumerate}
\end{frame}

\begin{frame}{Exercise 2 (b)}
	(b): Are these covariance functions stationary or isotropic? Justify your answer.
\end{frame}


\end{document}
