%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\newcommand{\fhx}[1]{\fh^{[#1]}(\xv)}
\newcommand{\rtilde}[1]{\tilde{r}^{[#1]}}
\newcommand{\Rt}[1]{R_t^{[#1]}}
\newcommand{\fhv}[1]{\tilde{\bm{f}}^{[#1]}}


\title[]{\textbf{Exercise of Supervised Learning: \\ Boosting Part 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{January 12, 2024}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage
\end{frame}

\begin{frame}{Exercise 1: Gradient Boosting}
	In the following, you assume that your outcome follows a $\log_2$-normal distribution with density function 
	$$p(y|f) = \frac{1}{y \sigma \sqrt{2\pi}} \exp\left( - \frac{(\log_2(y) - f)^2}{2\sigma^2} \right) \qquad \lhd$$ 
	where $\sigma = 1$. In other words, $\log_2(Y)$ follows a normal distribution. You observe $n = 3$ data points $\yv$ and want to model $f$ using features $\Xmat \in \mathbb{R}^{n \times p}$. You choose to use a gradient boosting tree algorithm. 
	
	(a) Derive the pseudo residuals based on the negative log-likelihood for the given distribution assumption.
\end{frame}

\begin{frame}{Solution to Exercise 1 (a)}
	(a) Derive the pseudo residuals based on the negative log-likelihood for the given distribution assumption.
	\begin{itemize}
		\item<2-> The loss is calulated by the NLL by: $$L(y, f) = - \ell(f) = - (const. - (\log_2(y) - f)^2 /2).$$
		\item<3-> The pseudo residuals are: $$\tilde{r} (f) = \partial L(y,f) / \partial f = (\log_2(y) - f).$$
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (b)}
	(b) Given only the 3 samples $\yv = (1, 2, 4)^T \qquad \lhd$ 
	
	and two features 
	$$\Xmat = (\xv_1, \xv_2) = 
		\begin{pmatrix}
			1 & 0 \\
			1 & 0 \\
			0 & 0 \\
		\end{pmatrix} \qquad \lhd
	$$
	explicitly derive or state with explanation
	\begin{enumerate}[(i)]
		\item the loss-optimal initial boosting model $\fhx{0}$,
		\item the pseudo residual $\rtilde{1}$,
		\item the regression stump $\Rt{1}, t = 1, 2$,
		\item the boosting model $\fhx{1}$ as well as 
		\item the pseduo residual $\rtilde{2}$
	\end{enumerate}
	for tree base learners with depth $d = 1$ (stumps) and a learning rate of $\alpha = 1$. 
\end{frame}

\begin{frame}{Solution to Exercise 1 (b)}
	(b) (i) Derive the loss-optimal intial boosting model $\fhx{1}$.
	\begin{itemize}
		\item<2-> We initialize $\fhx{0} = \argmin_{f^{[0]}} \sumin L(\yi, f^{[0]}(\xi))$.
		\item<3-> It can be easily seen that $\fhx{0} = \frac{1}{n} \sumin \log_2(\yi) = 1$, as it miminizes the squared error.
	\end{itemize}
\end{frame}

\begin{frame}{Solution to Exercise 1 (b): Continued}
	(b) (ii) Derive the pseudo residual $\rtilde{1}$.
	\begin{itemize}
		\item<2-> From (a) we know $\tilde{r} (f) = \partial L(y,f) / \partial f = (\log_2(y) - f)$.
		\item<3-> Denote $\fhv{0} = \left(\fh^{[0]}(\xv^{(1)}), \fh^{[0]}(\xv^{(2)}), \fh^{[0]}(\xv^{(3)}) \right)^T = (1, 1, 1)^T$
		\item<4-> So 
			\begin{align*}
				\rtilde{1} &= \left(\log_2(y^{(1)}), \log_2(y^{(2)}), \log_2(y^{(3)})\right)^T - \fhv{0} \\
				&= (0, 1, 2)^T - (1, 1, 1)^T \\
				&= (-1, 0, 1)^T.
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Solution to Exercise 1 (b): Continued}
	(b) (iii) Derive the regression stump $\Rt{1}, t = 1, 2$.
	\begin{itemize}
		\item<2-> $\Rt{1}, t = 1, 2$ will split using $\xv_1$, as $\xv_2$ carries no information.
		\item<3-> Note that $x_1^{(1)} = x_1^{(2)}$.
		\item<4-> Recall that $\rtilde{1} = (-1, 0, 1)^T$, and $\Rt{1}, t=1, 2$ aim to fit this pseudo residual.
		\item<5-> $R_1 = - 0.5 \cdot \id_{x_1 \geq 0.5}$, for which $-0.5$ stems from $\frac{1}{2}(\tilde{r}^{[1](1)} + \tilde{r}^{[1](2)})$ because $\xv_1^{(1)} , \xv_1^{(2)} \geq 0.5$.
		\item<6-> $R_2 = 1 \cdot \id_{x_1 < 0.5}$, for which $1$ stems from $\tilde{r}^{[1](3)}$ because only $\xv_1^{(3)} < 0.5$.
	\end{itemize}
\end{frame}

\begin{frame}{Solution to Exercise 1 (b): Continued}
	(b) (iv) Derive the boosting model $\fhx{1}$ (i.e., $\fhv{1}$).
	\begin{itemize}
		\item<2-> Recall that $R_1^{[1]} = - 0.5 \id_{x_1 \geq 0.5}$ and $R_2^{[1]} = 1 \cdot \id_{x_2 < 0.5}$ , and learning rate $\alpha = 1$.
		\item<3-> So the update direction given by the regression stump is $(-0.5, -0.5, 1)^T$.
		\item<4-> Therefore, 
			\begin{align*}
				\fhv{1} &= \fhv{0} + 1 \cdot (- 0.5, - 0.5, 1)^T \\
				&= (1, 1, 1)^t + (-0.5, -0.5, 1)^T \\
				&= (0.5, 0.5, 2)^T
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Solution to Exercise 1 (b): Continued}
	(b) (v) Derive the pseudo residual $\rtilde{2}$.
	\begin{itemize}
		\item Similar as the previous step, 
			\begin{align*}
				\rtilde{2} & = \left(log_2(y^{(1)}), \log_2(y^{(2)}), \log_2 (y^{(3)}) \right)^T - \fhv{1} \\
				&= (0, 1, 2)^T - (0.5, 0.5, 2)^T \\
				&= (-0.5, 0.5, 0)^T
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (c)}
	(c) What would happend in the second iteration of the previous boosting algorithm?
\end{frame}

\begin{frame}{Solution to Exercise 1 (c)}
	(c) What would happend in the second iteration of the previous boosting algorithm?

	\vspace{10pt}
	\begin{itemize}
		\item<2-> Recall that $\rtilde{2} = (-0.5, 0.5, 0)^T$, which needs to be fit by $R_1^{[2]}$ and $R_2^{[2]}$. 
		\item<3-> Similar as before, we split based on $\xv_1$, and $\xv^{(1)}, \xv^{(2)}$ goes to $R_1^{[2]}$, while $\xv^{(3)}$ goes to $R_2^{[2]}$.
		\item<4-> Therefore $R_1^{[2]} = \frac{-0.5 + 0.5}{2} \cdot \id_{x_1 \geq 0.5} = 0$, and $R_2^{[2]} = 0 \cdot \id_{x_2 < 0} = 0$.
		\item<5-> So the update direction is $(0, 0, 0)^T$, implying that no information can be used to improve the model.
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (d)}
	(d) If you are given more data points, but still the two binary feature vectors $\xv_1$ and $\xv_2$, what will happen as 
	\begin{enumerate}[(i)]
		\item $M$ grows
		\item $n$ grows
	\end{enumerate}
	in terms of model capacity (if $d$ is kept fixed)?
\end{frame}

\begin{frame}{Solution to Exercise 1 (d)}
	\begin{enumerate}[(i)]
		\item $M$ grows: capacity will increase and the algorithm may overfit.
		\item $n$ grows: capacity will stay the same and the algorithm may underfit.
	\end{enumerate}
\end{frame}


\end{document}
