%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}

\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\input{latex-math/ml-svm.tex}

\newcommand{\zv}{\bm{z}}
\newcommand{\betav}{\bm{\beta}}
\newcommand{\Kmat}{\bm{K}}
\newcommand{\linearphixi}{\thetab\top \phixi + \theta_0}
\newcommand{\phitxi}{\tilde{\phi}(\xi)}
\renewcommand{\phixi}{\phi(\xi)}
\newcommand{\psixiy}{\psi(\xi, y)}
\newcommand{\psixiyi}{\psi(\xi, \yi)}
\newcommand{\thetabt}{\tilde{\thetab}}
\newcommand{\betab}{\bm{\beta}}


\title[]{\textbf{Exercise of Supervised Learning: \\ SVM Part 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{December 23, 2024}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage
\end{frame}

\begin{frame}{Exercise 1: Kernelized Multiclass SVM}
	\small
	For a data set $\D = \{(\xv^{(1)}, y^{(1)}), \ldots, (\xv^{(n)}, y^{(n)}) \}$ with $\yi \in \Yspace = \{+1, -1 \}$, assume that we are provided with a suitable feature map $\phi: \Xspace \to \Phi$, where $\Phi \subset \mathbb{R}^d$. In the featureized SVM learning problem we are facing the following optimization problem:
	\begin{align*}
		\min_{\thetab, \theta_0, \sli} &\frac{1}{2} \thetab^\top \thetab + C \sumin \sli \\
		\text{s.t. } & \yi \left(\left\langle \thetab, \phixi \right\rangle  + \theta_0\right) \geq 1 - \sli \qquad \forall i \in \{1, \ldots, n \}, \\
		\text{and } & \sli \geq 0 \qquad i \in \{1, \ldots, n \},
	\end{align*}
	where $C \geq 0$ is some constant.
	
	(a) Argue that this is equivalent to the following ERM problem:
		\begin{align*}
			\riske(\thetab) = \frac{1}{2} ||\thetab||^2 + C\sumin \max(1 - \yi(\thetab^\top \phixi + \theta_0)), 0). \qquad \lhd 
		\end{align*}
		i.e., the regularized ERM problem for the hinge loss for the hypothesis space 
		\begin{align*}
			\Hspace = \{f: \Phi \to \mathbb{R} \ | \ f(\zv) = \thetab^\top \zv + \theta_0, \thetab \in \mathbb{R}^d, \theta_0 \in \mathbb{R} \}
		\end{align*}
\end{frame}

\begin{frame}{1(a): Rewriting the Optimization Target}
	\textbf{Optimization target:}
	\begin{align*}
	    \min_{\thetab, \theta_0, \zeta^{(i)}} &\frac{1}{2} \thetab^\top \thetab + C \sum_{i=1}^n \zeta^{(i)} \\
	    \text{s.t. } & \sli \geq 1 - y^{(i)} \left(\langle \thetab, \phi(\mathbf{x}^{(i)}) \rangle + \theta_0 \right), \quad \forall i, \\
	    \text{and } & \zeta^{(i)} \geq 0, \quad \forall i.
	\end{align*}
	
\end{frame}


\begin{frame}{1(a): Comparison between Optimization Target and $\mathcal{R}_{\text{emp}}$}
\small
	\textbf{Optimization target:}
	\begin{align*}
	    \min_{\thetab, \theta_0, \zeta^{(i)}} &\frac{1}{2} \thetab^\top \thetab + C \sum_{i=1}^n \zeta^{(i)} \\
	    \text{s.t. } & \sli \geq 1 - y^{(i)} \left(\langle \thetab, \phi(\mathbf{x}^{(i)}) \rangle + \theta_0 \right), \quad \forall i, \\
	    \text{and } & \zeta^{(i)} \geq 0, \quad \forall i.
	\end{align*}
	
	\uncover<2->{
    \textbf{Empirical risk:}
    \begin{align*}
        \mathcal{R}_{\text{emp}}(\thetab) = \frac{1}{2} \|\thetab\|^2 + C \sum_{i=1}^n \max(1 - y^{(i)}(\thetab^\top \phi(\mathbf{x}^{(i)}) + \theta_0), 0). \quad
    \end{align*}
    }
    
    \uncover<3->{
    \textbf{Observation:} Both contain $\frac{1}{2} \thetab^\top \thetab$ and $1 - y^{(i)}(\thetab^\top \phi(\mathbf{x}^{(i)}) + \theta_0)$. Both contain $C \sumin \ldots$ penalty terms.
    
    \textbf{Next:} Prove that $\sli$ equals $\max(\ldots)$ term.
    }
\end{frame}

\begin{frame}{1 (a): Prove $\sli$ Equals $\max(\ldots)$ Term}
	\small
	\uncover<1->{
	For each $i$, The constraints in the optimization problem:
	\begin{align*}
		\sli &\geq \underbrace{1 - y^{(i)} \left(\langle \thetab, \phi(\mathbf{x}^{(i)}) \rangle + \theta_0 \right)}_{\text{(i)}} \\
		\sli &\geq \underbrace{0}_{\text{(ii)}}
	\end{align*}
	$\sli \geq$ (i) and (ii) $\Rightarrow$ $\sli \geq $ the larger term in (i) and (ii) $\Rightarrow$ $\sli \geq \max(\text{(i)}, \text{(ii)})$.
	}
	
	\vspace{10pt}
	
	\uncover<2->{
	Therefore, the constraints translate to $\sli \geq \max\left(1 - y^{(i)} \left(\thetab^\top \phi(\mathbf{x}^{(i)}) + \theta_0 \right), 0 \right)$
	}
	\vspace{10pt}
	
	\uncover<3->{
	\textbf{Note:} Our target is to $\min_{\thetab, \theta_0, \zeta^{(i)}} \frac{1}{2} \thetab^\top \thetab + C \sum_{i=1}^n \zeta^{(i)}$, so smaller $\sli$ are preferred. $\leadsto$ Choose $ \sli = \max(\ldots, 0) $
	}
	
\end{frame}

\begin{frame}{1 (a): Choose $\sli = \max(\ldots, 0) $ }
The optimization target 

$$
\frac{1}{2} \thetab^\top \thetab + C \sumin \sli
$$
becomes 

$$
\frac{1}{2} \thetab^\top \thetab + C \sumin \max\left(1 - y^{(i)} \left(\thetab^\top \phi(\mathbf{x}^{(i)}) + \theta_0 \right), 0 \right)
$$

which is exactly $\riske$. \qed
\end{frame}


\begin{frame}{Exercise 1 (b)}
	(b) Now assume we deal with a multiclass classification problem with a data set $\D = \{(\xv^{(1)}, y^{(1)}), \ldots, (\xv^{(n)}, y^{(n)})\}$ such that $y^{(i)} \in \Yspace = \{1, \ldots, g \}$ for each $i \in \{1, \ldots, n \}$. In this case, we can derive a similar regularized ERM problem by using the multiclass hinge loss (see Exercse Sheet 4(b)):
	$$\risket = \frac{1}{2} || \thetab||^2 + C \sumin \sum_{y\neq \yi} \max (1 + \textcolor{red}{\thetabt}^\top \psi (\xi, y) - \textcolor{red}{\thetabt}^\top \psi(\xi, \yi), 0), $$
	where \textcolor{red}{$\thetabt := (\theta_0, \thetab^\top)^\top \in \mathbb{R}^{d + 1}$}, and $\psi: \Xspace \times \Yspace \to \mathbb{R}^{d \textcolor{red}{+1}}$ is a suitable (multiclass) feature map. Specify a $\psi$ such that this regularized multiclass ERM problem coincides with the regularized binary ERM problem in (a). \textcolor{red}{P.S.: Red colored text means the places different from the exercise. May be updated in the next version of the exercise.}
\end{frame}

\begin{frame}{1(b): A Closer Look Into the Multiclass Hinge Loss}
\small
\textbf{Goal:} Prove that Multiclass Hinge Loss resolves to the Binary Hinge Loss (a) in the binary case.
	\begin{itemize}
		\item<1-> \textbf{Class label encoding:} Binary: $\yi \in \{-1, +1\}$. Multiclass: $\yi \in \{1, \ldots, g\}$.
		\item<2-> \textbf{Penalty:} 
			\begin{itemize}
				\item Binary: $C \sumin \max\left(1 - y^{(i)} \left(\thetab^\top \phi(\mathbf{x}^{(i)}) + \theta_0 \right), 0 \right)$.
				\item Multiclass: $C \sumin \sum_{y\neq \yi} \max (1 + \thetab^\top \psi (\xi, y) - \thetab^\top \psi(\xi, \yi), 0)$.
			\end{itemize}
		\item<3-> Align class label encoding: $\yi \in \{1, 2\} \leadsto \yi \in \{-1, 1 \}$.
		\item<4-> $\sum_{y \neq \yi}$ means: $\yi = +1, y = -1$ or $\yi = -1, y = +1$.
		\item<5-> \textbf{Note} $\psi(\xi, \yi)$ takes both $\xi$ and $\yi$ as arguments, while $\phi(\xi)$ only operates on $\xi$.
		\item<6-> There is no $\theta_0$ in Multiclass Hinge Loss. \textbf{How to deal with $\theta_0$?} 
	\end{itemize}
	
\end{frame}

\begin{frame}{1(b): Define $\psi: \Xspace \times \Yspace \rightarrow \mathbb{R}^{d + 1}$}
	\begin{enumerate}
		\item<1-> Motivation: functional margin has form $y(\langle\phi(\xv), \thetab \rangle + \theta_0)$. We need it into a inner product $\langle \cdot, \cdot \rangle$.
		\item<2-> We need to merge $\theta_0$ into the inner product. We can add a dummy feature $1$ to $\phi(\xv)$, as $\langle\phi(\xv), \thetab \rangle + \theta_0 = \langle(1, \phi(\xv) )^\top,  (\theta_0, \thetab)^\top \rangle$. Define $\tilde{\phi}(\xv) = (1, \phi(\xv))^\top $, and $\tilde{\thetab} = (\theta_0, \thetab)^\top$.
		\item<3-> We can merge the coefficient $y$ into $\tilde{\phi}(\xv)$, obtaining $y\tilde{\phi}(\xv)$.
		\item<4-> We have transformed $y(\langle\phi(\xv), \thetab \rangle + \theta_0)$ into inner product $\langle y \tilde{\phi}(\xv), \thetabt \rangle$.
		\item<5-> Multiply with a magic number $\frac{1}{2}$. Consider $\psi(\xv, y) = \frac{1}{2} y \tilde{\phi}(\xv)$.
	\end{enumerate}
	
	\vspace{10pt}
	\uncover<6->{
	\textbf{Our next target:} Prove that in the binary case:
	
	$$
	\sum_{y\neq \yi} \max (1 + \thetabt^\top \psi (\xi, y) - \thetabt^\top \psi(\xi, \yi), 0)
	$$ 
	is equivalent to 
	$$
	\max(1 - \yi (\thetab^\top \phixi + \theta_0), 0)
	$$
	}
\end{frame}

\begin{frame}{1 (b): We need to reach $\max(1 - \yi (\thetab^\top \phixi + \theta_0), 0)$}
	\small
	
	\begin{enumerate}
		\item<1-> In the binary case, $\sum_{y\neq \yi} \max (1 + \thetabt^\top \psi (\xi, y) - \thetabt^\top \psi(\xi, \yi), 0)$ has \textbf{only one term}.
		\item<2-> The only term corresponds to $\yi = +1, y = -1$ or $\yi = -1, y = +1$.
	\end{enumerate}
	\vspace{10pt}
	
	\uncover<3->{
	Therefore, 
	\begin{align*}
		& 1 + \thetabt^\top \psixiy - \thetabt^\top \psixiyi \\
		&= 1 + \frac{1}{2} y \thetabt^\top \phitxi - \frac{1}{2} \yi \thetabt^\top \phitxi \\
		&= 1 + \frac{1}{2} \left(y - \yi \right) \thetabt^\top \phixi \\
		&= \begin{cases}
			1 +  \thetabt^\top \phitxi, \qquad &\text{if} \ \yi = - 1, y = +1 \\
			1 -  \thetabt^\top \phitxi, \qquad &\text{if} \ \yi = +1, y = -1 \\
		\end{cases} \\
		&= 1 - \yi \thetabt^\top \phitxi. \\
	\end{align*}
	}
\end{frame}

\begin{frame}{Solution to 1 (b): Continued}
	Thus, 
				\begin{align*}
					\risket &= \frac{1}{2}||\thetab||^2 + C \sumin \sum_{y \neq \yi} \max(1 + \thetabt^\top \psixiy - \thetabt^\top \psixiyi, 0) \\
					&= \frac{1}{2} ||\thetab||^2 + C \sumin \max(1 - \yi \thetabt^\top\phitxi, 0) \\
					&= \frac{1}{2} ||\thetab||^2 + C \sumin \max(1 - \yi (\thetab^\top \phixi + \theta_0), 0).
				\end{align*}
\end{frame}

\begin{frame}{Exercise 1 (c)}
	\footnotesize
	(c) Show that the regularized multiclass ERM problem in (b) can be written in the kernelized form:
	$$\frac{1}{2} \betav^\top \Kmat \betav + C\sumin \sum_{y \neq \yi} \max (1 + (\Kmat \beta)_{(i-i)g + y} - (\Kmat \betav)_{(i-1)g + \yi}), 0),$$
	where $\betav \in \mathbb{R}^{ng}$ and $\Kmat = \Xmat \Xmat^\top$ for $\Xmat \in \mathbb{R}^{ng \times (d \textcolor{red}{+1})}$ with row entries $\psi(\xi, y)^\top $ for $i = i, \ldots, n$, $y = 1, \ldots, g$, i.e., 
	$$\Xmat = \begin{pmatrix}
		\psi(\xv^{(1)}, 1)^\top & \\
		\psi(\xv^{(1)}, 2)^\top & \\
		\vdots & \\
		\psi(\xv^{(1)}, g)^\top &  \\
		\psi(\xv^{(2)}, 1)^\top & \\
		\vdots & \\
		\psi(\xv^{(n)}, g)^\top &  \\
	\end{pmatrix}.$$
	Here, $(\Kmat \betav)_{(i-1)g + y}$ denotes the $((i-1)g + y)$-th entry of the vector $\Kmat \betav$. \emph{Hint:} The representation theorems tells us that for the solution $\thetab^*$ of $\risket$ it holds that $\thetab^* \in \mathrm{span} \{(\psi(\xi, y))_{i=1, \ldots, n, y=1, \ldots, g} \}$
\end{frame}

\begin{frame}{1 (c): Express $||\thetab||_2^2$ with $\Kmat$ and $\betab$}

	$\thetab^* \in \mathrm{span} \{(\psi(\xi, y))_{i=1, \ldots, n, y=1, \ldots, g} \}$ means that $\thetab$ is a linear combination of the spanning bases,
	 
	\uncover<2->{
		i.e. $\thetab = \Xmat^\top \betab$ for $\betab \in \mathbb{R}^{ng}$ and 
		$$\Xmat = \begin{pmatrix}
			\psi(\xv^{(1)}, 1)^\top & \\
			\psi(\xv^{(1)}, 2)^\top & \\
			\vdots & \\
			\psi(\xv^{(1)}, g)^\top &  \\
			\psi(\xv^{(2)}, 1)^\top & \\
			\vdots & \\
			\psi(\xv^{(n)}, g)^\top &  \\
		\end{pmatrix}.
	$$
	}
	\uncover<3->{
		So for $\Kmat = \Xmat \Xmat^\top$, we obtain
		$$||\thetab||^2 = \thetab^\top \thetab = (\Xmat^\top \betab)^\top \Xmat^\top \betab = \betab^\top \Kmat \betab$$
	}
\end{frame}

\begin{frame}{1 (c): Express $\thetab^\top \psi(\xi, y)$ with $\Kmat$ and $\betab$}
Furthermore,
	$$\thetab^\top \psixiy - \thetab^\top \psixiyi = \betab^\top \Xmat \psixiy - \betab^\top\Xmat\psixiyi$$
	Note that the result is a scalar.
	\begin{itemize}
		\item<2-> Recall that $\Kmat = \Xmat \Xmat^\top$. 
		\item<3-> $\psixiy$ is the $((i - 1) g + y)$-th row of $\Xmat$. (Similar argument for $\psixiyi$)
		\item<4-> So, $\Xmat \psixiy$ is the $((i - 1) g + y)$-th row/column of $\Kmat = \Xmat \Xmat^\top$ (symmetric).
		\item<5-> So, the inner product $\betab^\top (\Xmat \psixiy)$ is equivalent to: first compute $\Kmat \betab$, and then retrieve the entry in the $((i - 1) g + y)$-th row.
	\end{itemize}
	\uncover<5->{
		Therefore, 
		\begin{align*}
			\risket &= \frac{1}{2} ||\thetab||^2 + C\sumin \sum_{y \neq \yi} \max(1 + \thetab^\top \psixiy - \thetab^\top \psixiyi, 0) \\
			&= \frac{1}{2} \betab^\top \Kmat \betab + \sumin \sum_{y \neq \yi} \max(1 + (\Kmat \betab)_{(i-1)g + y} - (\Kmat \betab)_{(i-1)g + \yi}), 0)
		\end{align*}
	}
\end{frame}


\begin{frame}{Exercise 2: Kernel Trick}
	The polynomial kernel is defined as 
	$$k(x, \xt) = (x^\top \xt + b)^d.$$
	Furthermore, assume that $x \in \mathbb{R}^2$ and $d = 2$.
	(a) Derive the explicit feature map $\phi$ taking into account that the following equation holds:
	$$k(x, \xt) = \langle \phi(x), \phi(\xt) \rangle$$
	
\end{frame}

\begin{frame}{Solution to 2 (a)}
	\begin{align*}
		k(x, \xt) &= (x^\top \xt + b)^2 = 
		\left( 
			\begin{pmatrix}
				x_1 \\
				x_2
			\end{pmatrix}^\top 
			\begin{pmatrix}
				\xt_1 \\
				\xt_2 \\
			\end{pmatrix}
			+ b 
		\right)^2 \\
		\only<2->{
			&= (x_1 \xt_1 + x_2 \xt_2 + b)^2 \\}
		\only<3->{
			&= x_1^2 \xt_1^2 + 2 x_1 \xt_1 x_2 \xt_2 + x_2^2 \xt_2^2 + 2b x_1 \xt_1 + 2b x_2 \xt_2 + b^2 \\}
		\only<4->{
			&= \left\langle 
			\begin{pmatrix}
				x_1^2 \\
				\sqrt{2} x_1 x_2 \\
				x_2^2 \\
				\sqrt{2b} x_1 \\
				\sqrt{2b} x_2 \\
				b \\
			\end{pmatrix}, 
			\begin{pmatrix}
				\xt_1^2 \\
				\sqrt{2} \xt_1 \xt_2 \\
				\xt_2^2 \\
				\sqrt{2b} \xt_1 \\
				\sqrt{2b} \xt_2 \\
				b \\
			\end{pmatrix} 
			\right\rangle \\
			&= \langle \phi(x), \phi(\xt) \rangle
		}
	\end{align*}
\end{frame}

\begin{frame}{Exercise 2 (b)}
	(b) Describe the main differences between the kernel method and the explicit feature map.
	
	\vspace{10pt}
	\uncover<2->{
		\textbf{Solution:} Using the kernel method reduces the compuational costs of computing the scalar product in the higher-dimensional features space after calculating the feature map.
	}
\end{frame}



\end{document}
