%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\input{latex-math/ml-svm.tex}

\newcommand{\zv}{\bm{z}}
\newcommand{\betav}{\bm{\beta}}
\newcommand{\Kmat}{\bm{K}}
\newcommand{\linearphixi}{\thetab^T \phixi + \theta_0}
\newcommand{\phitxi}{\tilde{\phi}(\xi)}
\renewcommand{\phixi}{\phi(\xi)}
\newcommand{\psixiy}{\psi(\xi, y)}
\newcommand{\psixiyi}{\psi(\xi, \yi)}
\newcommand{\thetabt}{\tilde{\thetab}}
\newcommand{\betab}{\bm{\beta}}


\title[]{\textbf{Exercise of Supervised Learning: \\ SVM Part 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{December 15, 2023}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}

\begin{frame}{Exercise 1: Kernelized Multiclass SVM}
	\small
	For a data set $\D = \{(\xv^{(1)}, y^{(1)}), \ldots, (\xv^{(n)}, y^{(n)}) \}$ with $\yi \in \Yspace = \{+1, -1 \}$, assume that we are provided with a suitable feature map $\phi: \Xspace \to \Phi$, where $\Phi \subset \mathbb{R}^d$. In the featureized SVM learning problem we are facing the following optimization problem:
	\begin{align*}
		\min_{\thetab, \theta_0, \sli} &\frac{1}{2} \thetab^T \thetab + C \sumin \sli \\
		\text{s.t. } & \yi \left(\left\langle \thetab, \phixi \right\rangle  + \theta_0\right) \geq 1 - \sli \qquad \forall i \in \{1, \ldots, n \}, \\
		\text{and } & \sli \geq 0 \qquad i \in \{1, \ldots, n \},
	\end{align*}
	where $C \geq 0$ is some constant.
	
	(a) Argue that this is equivalent to the following ERM problem:
		\begin{align*}
			\riske(\thetab) = \frac{1}{2} ||\thetab||^2 + C\sumin \max(1 - \yi(\thetab^T \phixi + \theta_0)), 0). \qquad \lhd 
		\end{align*}
		i.e., the regularized ERM problem for the hinge loss for the hypothesis space 
		\begin{align*}
			\Hspace = \{f: \Phi \to \mathbb{R} \ | \ f(\zv) = \thetab^T \zv + \theta_0, \thetab \in \mathbb{R}^d, \theta_0 \in \mathbb{R} \}
		\end{align*}
\end{frame}

\begin{frame}{Solution to Exercise 1 (a)}

	\begin{enumerate}
		\item<1-> Identify that: $\langle \thetab, \phixi \rangle + \theta_0 = \linearphixi$.
		\item<2-> Check the conditions $\yi (\linearphixi) \geq 1 - \sli$ and $\sli \geq 0$.
		\begin{itemize}
			\item<3-> Case 1: if $\yi (\linearphixi) \geq 1$, then $\sli = 0$. \qquad $\rhd$
			\item<4-> Case 2: if $\yi (\linearphixi) < 1$, then $\sli = 1 - \yi(\linearphixi) > 0$. \qquad $\rhd$
		\end{itemize}
		\uncover<5->{
			Combining both cases, we can write $$\sli = \max(0, 1 - \yi(\linearphixi))$$
			Plug in to the primal problem we can prove that it is equivalent to the $\risket$ using hinge loss.
		}
	\end{enumerate}
\end{frame}

\begin{frame}{Exercise 1 (b)}
	(b) Now assume we deal with a multiclass classification problem with a data set $\D = \{(\xv^{(1)}, y^{(1)}), \ldots, (\xv^{(n)}, y^{(n)})\}$ such that $y^{(i)} \in \Yspace = \{1, \ldots, g \}$ for each $i \in \{1, \ldots, n \}$. In this case, we can derive a similar regularized ERM problem by using the multiclass hinge loss (see Exercse Sheet 4(b)):
	$$\risket = \frac{1}{2} || \thetab||^2 + C \sumin \sum_{y\neq \yi} \max (1 + \thetab^T \psi (\xi, y) - \thetab^T \psi(\xi, \yi), 0), \quad \lhd $$
	where $\psi: \Xspace \times \Yspace \to \mathbb{R}^d$ is a suitable (multiclass) feature map. Specify a $\psi$ such that this regularized multiclass ERM problem coincides with the regularized binary ERM problem in (a).
\end{frame}

\begin{frame}{Solution to 1 (b)}
	\begin{enumerate}
		\item<1-> Motivation: functional margin has form $y(\langle\phi(\xv), \thetab \rangle + \theta_0)$. We can turn it into a inner product, e.g. something like $\langle \cdot, \thetab\rangle$.
		\item<2-> We need to merge $\theta_0$ into the inner product. We can add a dummy feature $1$ to $\phi(\xv)$, as $\langle\phi(\xv), \thetab \rangle + \theta_0 = \langle(1, \phi(\xv) )^T,  (\theta_0, \thetab)^T \rangle$. Define $\tilde{\phi}(\xv) = (1, \phi(\xv))^T $, and $\tilde{\thetab} = (\theta_0, \thetab)^T$.
		\item<3-> We can merge the coefficient $y$ into $\tilde{\phi}(\xv)$, obtaining $y\tilde{\phi}(\xv)$.
		\item<4-> We have transformed $y(\langle\phi(\xv), \thetab \rangle + \theta_0)$ into inner product $\langle y \tilde{\phi}(\xv), \thetabt \rangle$.
		\item<5-> Multiply with a magic number $\frac{1}{2}$. Consider $\psi(\xv, y) = \frac{1}{2} y \tilde{\phi}(\xv)$.
	\end{enumerate}
\end{frame}

\begin{frame}{Solution to 1(b): Continued}
	\begin{enumerate}
		\setcounter{enumi}{5}
		\item Then, for $y \neq \yi$, it follows that 
				\begin{align*}
					& 1 + \thetabt^T \psixiy - \thetab^T \psixiyi \\
					&= 1 + \frac{1}{2} y \thetabt^T \phitxi - \frac{1}{2} \yi \thetabt^T \phitxi \\
					&= 1 + \frac{1}{2} \left(y - \yi \right) \thetabt^T \phitxi \\
					&= \begin{cases}
						1 + \thetabt^T \phitxi, \qquad &\text{if} \ \yi = - 1 \\
						1 - \thetabt^T \phitxi, \qquad &\text{if} \ \yi = +1 \\
					\end{cases} \\
					&= 1 - \yi \thetabt^T \phitxi. \\
				\end{align*}
	\end{enumerate}
\end{frame}

\begin{frame}{Solution to 1 (b): Continued}
	\begin{enumerate}
		\setcounter{enumi}{6}
		\item Thus, 
				\begin{align*}
					\risket &= \frac{1}{2}||\thetab||^2 + C \sumin \sum_{y \neq \yi} \max(1 + \thetabt^T \psixiy - \thetabt^T \psixiyi, 0) \\
					&= \frac{1}{2} ||\thetab||^2 + C \sumin \max(1 - \yi \thetabt^T\phitxi, 0) \\
					&= \frac{1}{2} ||\thetab||^2 + C \sumin \max(1 - \yi (\thetab^T \phixi + \theta_0), 0).
				\end{align*}
	\end{enumerate}
\end{frame}

\begin{frame}{Exercise 1 (c)}
	\footnotesize
	(c) Show that the regularized multiclass ERM problem in (b) can be written in the kernelized form:
	$$\frac{1}{2} \betav^T \Kmat \betav + C\sumin \sum_{y \neq \yi} \max (1 + (\Kmat \beta)_{(i-i)g + y} - (\Kmat \betav)_{(i-1)g + \yi}), 0),$$
	where $\betav \in \mathbb{R}^{ng}$ and $\Kmat = \Xmat \Xmat^T$ for $\Xmat \in \mathbb{R}^{ng \times d} $ with row entries $\psi(\xi, y)^T $ for $i = i, \ldots, n$, $y = 1, \ldots, g$, i.e., 
	$$\Xmat = \begin{pmatrix}
		\psi(\xv^{(1)}, 1)^T & \\
		\psi(\xv^{(1)}, 2)^T & \\
		\vdots & \\
		\psi(\xv^{(1)}, g)^T &  \\
		\psi(\xv^{(2)}, 1)^T & \\
		\vdots & \\
		\psi(\xv^{(n)}, g)^T &  \\
	\end{pmatrix}. \qquad \lhd$$
	Here, $(\Kmat \betav)_{(i-1)g + y}$ denotes the $((i-1)g + y)$-th entry of the vector $\Kmat \betav$. \emph{Hint:} The representation theorems tells us that for the solution $\thetab^*$ of $\risket$ it holds that $\thetab^* \in \mathrm{span} \{(\psi(\xi, y))_{i=1, \ldots, n, y=1, \ldots, g} \}$
\end{frame}

\begin{frame}{Solution to Exercise 1 (c)}

	$\thetab^* \in \mathrm{span} \{(\psi(\xi, y))_{i=1, \ldots, n, y=1, \ldots, g} \}$ means that $\thetab$ is a linear combination of the spanning bases,
	 
	\uncover<2->{
		i.e. $\thetab = \Xmat^T \betab$ for $\betab \in \mathbb{R}^{ng}$ and 
		$$\Xmat = \begin{pmatrix}
			\psi(\xv^{(1)}, 1)^T & \\
			\psi(\xv^{(1)}, 2)^T & \\
			\vdots & \\
			\psi(\xv^{(1)}, g)^T &  \\
			\psi(\xv^{(2)}, 1)^T & \\
			\vdots & \\
			\psi(\xv^{(n)}, g)^T &  \\
		\end{pmatrix}.
	$$
	}
	\uncover<3->{
		So for $\Kmat = \Xmat \Xmat^T$, we obtain
		$$||\thetab||^2 = \thetab^T \thetab = (\Xmat^T \betab)^T \Xmat^T \betab = \betab^T \Kmat \betab$$
	}
\end{frame}

\begin{frame}{Solution to Exercise 1 (c): Continued}
Furthermore,
	$$\thetab^T \psixiy - \thetab^T \psixiyi = \betab^T \Xmat \psixiy - \beta^T\Xmat\psixiyi$$
	Note that the result is a scalar.
	\begin{itemize}
		\item<2-> Recall that $\Kmat = \Xmat \Xmat^T$. 
		\item<3-> $\psixiy$ is the $((i - 1) g + y)$-th row of $\Xmat$. (Similar argument for $\psixiyi$)
		\item<4-> So, $\Xmat \psixiy$ is the $((i - 1) g + y)$-th row/column of $\Kmat = \Xmat \Xmat^T$ (symmetric).
		\item<5-> So, the inner product $\betab^T (\Xmat \psixiy)$ is equivalent to: first compute $\Kmat \betab$, and then retrieve the entry in the $((i - 1) g + y)$-th row.
	\end{itemize}
	\uncover<5->{
		Therefore, 
		\begin{align*}
			\risket &= \frac{1}{2} ||\thetab||^2 + C\sumin \sum_{y \neq \yi} \max(1 + \thetab^T \psixiy - \thetab^T \psixiyi, 0) \\
			&= \frac{1}{2} \betab^T \Kmat \betab + \sumin \sum_{y \neq \yi} \max(1 + (\Kmat \betab)_{(i-1)g + y} - (\Kmat \betab)_{(i-1)g + \yi}), 0)
		\end{align*}
	}
\end{frame}


\begin{frame}{Exercise 2: Kernel Trick}
	The polynomial kernel is defined as 
	$$k(x, \xt) = (x^T \xt + b)^d.$$
	Furthermore, assume that $x \in \mathbb{R}^2$ and $d = 2$.
	(a) Derive the explicit feature map $\phi$ taking into account that the following equation holds:
	$$k(x, \xt) = \langle \phi(x), \phi(\xt) \rangle$$
	
\end{frame}

\begin{frame}{Solution to 2 (a)}
	\begin{align*}
		k(x, \xt) &= (x^T \xt + b)^T = 
		\left( 
			\begin{pmatrix}
				x_1 \\
				x_2
			\end{pmatrix}^T 
			\begin{pmatrix}
				\xt_1 \\
				\xt_2 \\
			\end{pmatrix}
			+ b 
		\right)^2 \\
		\only<2->{
			&= (x_1 \xt_1 + x_2 \xt_2 + b)^2 \\}
		\only<3->{
			&= x_1^2 \xt_1^2 + 2 x_1 \xt_1 x_2 \xt_2 + x_2^2 \xt_2^2 + 2b x_1 \xt_1 + 2b x_2 \xt_2 + b^2 \\}
		\only<4->{
			&= \left\langle 
			\begin{pmatrix}
				x_1^2 \\
				\sqrt{2} x_1 x_2 \\
				x_2^2 \\
				\sqrt{2b} x_1 \\
				\sqrt{2b} x_2 \\
				b \\
			\end{pmatrix}, 
			\begin{pmatrix}
				\xt_1^2 \\
				\sqrt{2} \xt_1 \xt_2 \\
				\xt_2^2 \\
				\sqrt{2b} \xt_1 \\
				\sqrt{2b} \xt_2 \\
				b \\
			\end{pmatrix} 
			\right\rangle \\
			&= \langle \phi(x), \phi(\xt) \rangle
		}
	\end{align*}
\end{frame}

\begin{frame}{Exercise 2 (b)}
	(b) Describe the main differences between the kernel method and the explicit feature map.
	
	\vspace{10pt}
	\uncover<2->{
		\textbf{Solution:} Using the kernel method reduces the compuational costs of computing the scalar product in the higher-dimensional features space after calculating the feature map.
	}
\end{frame}



\end{document}
