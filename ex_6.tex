%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}
\input{latex-math/ml-infotheory.tex}

\newcommand{\HX}{H(X)}
\newcommand{\HY}{H(Y)}
\newcommand{\HXY}{H(X, Y)}
\newcommand{\HXcY}{H(X | Y)}
\newcommand{\HYcX}{H(Y|X)}
\newcommand{\IXY}{I(X;Y)}


\title[]{\textbf{Supervised Learning: Exercise for \\ Information Theory Part 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{Date}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}


\begin{frame}{Exercise 1: Entropy}
	A fair \textbf{die} is rolled at the same time as a fair \textbf{coin} is tossed. Let $A$ be the number on the upper surface of the dice and let $B$ describe the outcome of the coin toss, where
	$$
		B = \begin{cases}
			1, \quad &\text{head,} \\
			0, \quad &\text{tail.}
		\end{cases}
	$$
	Two random variables $X$ and $Y$ are given by $X = A + B$ and $Y = A - B$, respectively.
	
	(a) Calculate the entropies $\HX$ and $\HY$, the conditional entropies $\HYcX$ and $\HXcY$, the joint entropy $\HXY$ and the mutual information $\IXY$.
\end{frame}

\begin{frame}{Solution to Exercise 1 (a)}
	\begin{enumerate}
		\item Let $a, b , x, y$ be the realizations of $A, B, X, Y$, rspectively.
		\item \textbf{If we have observed $x$ and $y$, then we can calculate the observed $a$ and $b$.}
			
			Since: $x = a + b$ and $y = a - b$ yields $a = \frac{x + y}{2}$ and $b = \frac{x - y}{2}$.
		\item In other words, a pair $(x, y)$ is uniquely associated with a pair $(a, b)$.
		\item For each pair $(a, b) \in \{0, 1, \ldots, 6 \} \times \{0, 1\}$, it holds that $p_{AB}(a, b) = \frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}$.
		\item Therefore, $p_{XY}(x, y) = p_{AB}(a, b) = \frac{1}{12}$ for all $(x, y)$.
		\item So, the joint entropy
			\begin{align*}
				\HXY &= - \sum_{x, y} p_{XY}(x, y) \log_2 p_{XY}(x, y) = -12 \cdot \frac{1}{12} \log_2 \frac{1}{12} \\
				&= 2 + \log_2 3.
			\end{align*}

	\end{enumerate}
	
\end{frame}

\begin{frame}{Solution to Exercise 1 (a): Continued}
	\small
	Next, we compute $\HX$ and $\HY$. We enumerate all the possible $(a, b)$ events.
	\vspace{10pt}
	 
	\begin{minipage}{0.49\textwidth}
		\centering
		\begin{tabular}{c c c}
			\hline
			$x$ & events $(a, b)$ & $p_X(x)$ \\
			\hline
			1 & (1, 0) & 1/12 \\
			2 & (2, 0), (1, 1) & 1/6 \\
			3 & (3, 0), (2, 1) & 1/6 \\
			4 & (4, 0), (3, 1) & 1/6 \\
			5 & (5, 0), (4, 1) & 1/6 \\
			6 & (6, 0), (5, 1) & 1/6 \\
			7 & (6, 1) & 1/12 \\
			\hline
		\end{tabular}
		
		\begin{align*}
			\HX &= \sum_x p_X(x) \log_2 p_X(x) \\
				&= -2 \cdot \frac{1}{12} \log_2 \frac{1}{12} - 5\cdot \frac{1}{6} \log_2 \frac{1}{6} \\
				&= \frac{7}{6} + \log_2 3.
		\end{align*}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\begin{tabular}{c c c}
			\hline
			$y$ & events $(a, b)$ & $p_Y(y)$ \\
			\hline
			0 & (1, 1) & 1/12 \\
			1 & (1, 0), (2, 1) & 1/6 \\
			2 & (2, 0), (3, 1) & 1/6 \\
			3 & (3, 0), (4, 1) & 1/6 \\
			4 & (4, 0), (5, 1) & 1/6 \\
			5 & (5, 0), (6, 1) & 1/6 \\
			6 & (6, 0) & 1/12 \\
			\hline
		\end{tabular}
		
		\begin{align*}
			\HY &= \sum_y p_Y(x) \log_2 p_Y(y) \\
				&= -2 \cdot \frac{1}{12} \log_2 \frac{1}{12} - 5\cdot \frac{1}{6} \log_2 \frac{1}{6} \\
				&= \frac{7}{6} + \log_2 3.
		\end{align*}
	\end{minipage}

\end{frame}

\begin{frame}{Solution to Exercise 1 (a): Continued}
	The conditional entropies are
	\begin{align*}
		\HXcY &= \HXY - \HY = 2 + \log_2 3 - \frac{7}{6} - \log_2 3 = \frac{5}{6} \\
		\HYcX &= \HXY - \HX = 2 + \log_2 3 - \frac{7}{6} - \log_2 3 = \frac{5}{6} \\
	\end{align*}
	
	The mutual information $\IXY$ can be determined according to
	\begin{align*}
		\IXY = \HX - \HXY = \frac{7}{6} + \log_2 3 - \frac{5}{6} = \frac{1}{3} + \log_2 3.
	\end{align*}
\end{frame}


\begin{frame}{Exercise 1: Question (b)}
	(b) Show that, for independent discrete random variables $X$ and $Y$,
	$$I(X; X + Y) - I(Y; X + Y) = \HX - \HY$$
\end{frame}

\begin{frame}{Solution to Exercise 1 (b)}
\small
	\begin{align*}
		I(X; X+ Y) - I(Y; X+Y) 
		&= \HX - H(X | X + Y) - \HY + H(Y | X + Y) \\
		&= \HX - \HY  + (H(Y| X+Y) - H(X| X+Y)) \\
		&= \HX - \HY + \\
		&\qquad (H(Y, X+Y) - H(X+Y) - H(X, X+Y) + H(X+Y)) \\
		&= \HX - \HY + \underbrace{H(Y, X+Y) - H(X, X+Y)}_{=0} \\
		&= \HX - \HY
	\end{align*}

Note that if we observe $x+y$, and assume we also observe $x$, we can infer $y$.	 

In other words, \textbf{each pair $(x+y, x)$ has the same probability as $(x+y, y)$. Therefore, $H(Y, X+Y) = H(X, X+Y)$.}

\end{frame}

\begin{frame}{Exercise 2: Mutual Information of Three Variables}
	Let $X, Y, Z$ be three discrete random variables. The mutual information of $X, Y,$ and $Z$ is defined as:
	$$I(X; Y; Z) = \sum_x \sum_y \sum_z p(x, y, z) \log \left( \frac{p(x,y) p(x,z) p(y,z)}{p(x) p(y) p(z) p(x,y,z)}\right).$$
	(a) Prove the lemma:  $$I(X; Y; Z) = I(X; Y) - I(X; Y |Z).$$ Note that the conditional information is defined as: $$I(X; Y | Z) = \sum_z \sum_x \sum_y p(z) p(x, y |z) \log \frac{p(x, y | z)}{p(x|z) p(y|z)}.$$
\end{frame}


\begin{frame}{Solution to Question 2 (a)}
	\small
	According to the definition of mutual information, 
	\begin{align*}
		I(X; Y) - I(X; Y | Z) 
		&= \sum_x \sum_y p(x, y) \log \frac{p(x, y)}{p(x) p(y)} - \sum_z \sum_x \sum_y p(z) p(x, y |z) \log \frac{p(x, y | z)}{p(x|z) p(y|z)} \\
		&= \sum_x \sum_y \sum_z p(x,y,z) \log \frac{p(x, y)}{p(x) p(y)} - \\
		& \qquad \sum_z \sum_x \sum_y p(z) p(x, y |z) \log \frac{p(x, y | z) p(z)^2}{p(x|z) p(y|z) p(z)^2} \\
		&= \sum_x \sum_y \sum_z p(x,y,z) \log \frac{p(x, y)}{p(x) p(y)} - \sum_z \sum_x \sum_y p(x, y, z) \log \frac{p(x, y, z) p(z)}{p(x, z) p(y,z)} \\
		&= \sum_x \sum_y \sum_z p(x,y,z) \log \left(\frac{p(x, y) p(x, z) p(y,z)}{p(x) p(y) p(z) p(x,y,z)} \right) \\
		&= I(X;Y;Z). 
	\end{align*}
\end{frame}

\begin{frame}{Exercise 2: Question (b)}
(b) Prove the following relation with the above lemma:
\begin{align*}
	I(X; Y) = I(X; Y |Z) + I(Y; Z) - I(Y; Z | X)
\end{align*}

Recall the lemma: $I(X; Y) - I(X; Y | Z)  = I(X; Y; Z)$

	
\end{frame}

\begin{frame}{Solution to Question 2 (b)}
	Using the lemma we just proved, we obtain:
	\begin{align*}
		& I(X; Y |Z) + I(Y; Z) - I(Y; Z | X) \\
		&= \IXY - I(X; Y; Z)  + I(Y; Z) - I(Y;Z) + I(X;Y;Z) \\
		&= \IXY 
	\end{align*}
\end{frame}

\end{document}
