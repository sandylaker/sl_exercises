%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}

\newcommand{\Ex}{\mathbb{E}_{\xv}}
\newcommand{\Eyx}{\mathbb{E}_{y|\xv}}
\newcommand{\Prob}{\mathbb{P}}

\title[]{\textbf{Supervised Learning: Exercise 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{November 3, 2023}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}


\begin{frame}{Exercise 1: Risk Minimizers for 0-1-Loss}
	Consider the classification learning setting, i.e., $\Yspace = \{1, \ldots, g\}$, and the hypothetis space is $\Hspace = \{h: \Xspace \rightarrow \Yspace \}$. The loss function of interest is the 0-1-loss:
	$$L(y, h(\xv)) = \id_{y \neq h(\xv)} = \begin{cases}
		1, \quad &\text{if} y \neq h(\xv), \\
		0, \quad &\text{if} y = h(\xv).
	\end{cases}
	\qquad \lhd
	$$
	(a) Consider the hypothesis space of constant models $\Hspace = \{h: \Xspace \rightarrow \Yspace | h(\xv) = \thetab \in \Yspace \ \forall \xv \in \Xspace \}$, where $\Xspace$ is the feature space. Show that 
	$$
		\hat{h}(\xv) = \mathrm{mode} \left\{\yi \right\}
	$$
	is the empirical risk minimizer for the 0-1-loss in this case.
\end{frame}

\begin{frame}{Solution to Question (a)}
\small
The empirical risk is
	\begin{align*}
		\riske(h) &= \sumin \id_{\yi \neq h(\xi)} \\
		&=\sumin 1 - \id_{\yi = h(\xi)} \quad \rhd \\
	\end{align*}

Therefore
\begin{align*}
	\argmin_{h \in \Hspace} \riske(h) 
	\only<1->{&= \argmin_{h \in \Hspace} \sumin 1 - \id_{\yi = h(\xi)} \\} 
	\only<2->{&= \argmax_{h \in \Hspace} \sumin \id_{\yi = h(\xi)} \\}
	\only<3->{&= \argmax_{\thetab \in \Yspace} \sumin \id_{\yi = \thetab} =\mathrm{mode} \left\{ \yi \right\}}
\end{align*}
	
\end{frame}

\begin{frame}{Question (b)}
	(b) What is the optimal constant model interms of the (theoretical) risk for the 0-1-loss and what is its risk?
	\vspace{10pt}
	
	\uncover<2->{
	Constant model: $$\hx = \theta$$
	}
\end{frame}

\begin{frame}{Solution to Question (b)}
	\small
	\begin{align*}
		\risk_L(h) &= \int_y \int_\xv \id_{y \neq h(\xv)} p(\xv, y) \mathrm{d}\xv \mathrm{d}y \\
		\only<2->{&= \int_y \int_\xv \id_{y \neq \thetab} p(\xv, y) \mathrm{d}\xv \mathrm{d}y \\}
		\only<3->{&= \int_y \id_{y \neq \thetab} p(y) \mathrm{d}y \\}
		\only<4->{&= \int_y \left( 1 - \id_{y = \thetab} \right) p(y) \mathrm{d}y = 1 - \int_y \id_{y=\thetab} p(y) \mathrm{d}y }
	\end{align*}
	\uncover<5->{
		Therefore, $\argmin_{h} \risk_L(h) = \argmax_{\thetab \in \Yspace} \int_y \id_{y = \thetab} p(y) \mathrm{d} y$. 
	}
	\uncover<6->{
		Futhermore, $\Yspace = \{1, \ldots, g\}$, 
		it follows that $\argmax_{\thetab \in \Yspace} \int_y \id_{y = \thetab} p(y) \mathrm{d} y = \argmax_{\thetab \in \Yspace} \sumjg \id_{\thetab = j} \Prob (y=j)$. (Show example.)
	}
	
	\uncover<7->{
		Hence, the optimal constant model for the \textbf{theorerical} risk is 
		$$\bar{h}(\xv) = \argmax_{l \in \Yspace} \Prob(y = l)$$
	}
\end{frame}

\begin{frame}{Solution to Question (b): Continued}
	\small
	Before we compute $\risk_{L}(\bar{h})$, we write 0-1-loss as follows:
	$$L(y, h(\xv)) = \id_{y \neq h(\xv)} = \sum_{k \in \Yspace} \id_{y=k} \id_{k\neq h(\xv)} = \sum_{k \in \Yspace} L(k, h(x)).$$
	Then, the risk of $\bar{h}$ is 
	\begin{align*}
		\only<2->{
			\risk_L(\bar{h})
			&= \Exy[L(y, \bar{h}(\xv))] \\
		}
		\only<3->{&= \Ex\left[ \Eyx \left[L(y, \bar{h}(\xv)) \ | \ \xv \right] \right] \\}
		\only<4->{&= \Ex\left[ \Eyx \left[\sum_{k \in \Yspace} \id_{y=k} L(k, \bar{h}(\xv)) \ | \ \xv \right] \right] \\}
		\only<5->{&= \Ex\left[ \sum_{k \in \Yspace} L(k, \bar{h}(\xv)) \Eyx \left[\id_{y=k}  \ | \ \xv \right] \right] \quad \rhd \\}
	\end{align*}
\end{frame}

\begin{frame}{Solution to Question (b): Continued}
	\small
	\begin{align*}
		\risk_L(\bar{h})
		&= \Ex\left[ \sum_{k \in \Yspace} L(k, \bar{h}(\xv)) \Eyx \left[\id_{y=k}  \ | \ \xv \right] \right] \\
		&= \Ex\left[ \sum_{k \in \Yspace} L(k, \bar{h}(\xv)) \Prob(y = k \ | \ \xv) \right] \\
		&= \sum_{k \in \Yspace} L(k, \bar{h}(\xv)) \Ex \left[ \Prob(y = k \ | \ \xv) \right] \\
		& = \sum_{k \in \Yspace} L(k, \bar{h}(\xv)) \Prob(y = k) \\
		&= \sum_{k \in \Yspace} \id_{k \neq \bar{h}(\xv)} \Prob(y=k) \\
		&=  \sum_{k \in \Yspace} \id_{k \neq \argmax_{l \in \Yspace} \Prob(y=l)} \Prob(y=k) \\
		&= 1 - \max_{l \in \Yspace} \Prob(y=l).
	\end{align*}
\end{frame}

\begin{frame}{Question (c)}
	(c) Derive the approximation error if the hypothesis space $\Hspace$ consists of the \textbf{constant models}.
	\vspace{10pt}
	
	Recall that the approximation error is defined as 
	$$\inf_{h \in \Hspace} \risk_L(h) - \riskbayes_L$$
\end{frame}

\begin{frame}{Solution to (c)}
	\begin{align*}
		\inf_{h \in \Hspace} \risk_L(h) - \riskbayes_L 
		&= \risk_L(\bar{h}) - \riskbayes_L \\
		&= (1 - \max_{l \in \Yspace} \Prob(y=l)) - (1 - \Ex[\max_{l \in \Yspace} \Prob(y = l | \xv)]) \\
		&= \Ex[\max_{l \in \Yspace} \Prob(y = l | \xv)] - \max_{l \in \Yspace} \Prob(y=l).
	\end{align*}
\end{frame}

\begin{frame}{Question (d)}
	(d) Assume now $g=2$ (binary classification) and consider now the hypothesis space of probabilistic classifiers $\Hspace = \{\pi: \Xspace \to [0, 1] \}$, that is, $\pi(\xv)$ (or $1 - \pi(\xv)$) is an estimate of the posterior distribution $p_{y | \xv} (1 | \xv)$ (or $p_{y | \xv} (0 | \xv)$). Furthermore, consider the probabilistic 0-1-loss
	$$
	L(y, \pi(\xv)) = \begin{cases}
		1, \quad & \text{if ($\pi(\xv) \geq 1/2$ and $y=0$) or ($\pi(\xv) < 1/ 2$ and $y = 1$),} \\
		0, \quad & \text{else.}
	\end{cases}
	$$
	Is the minimum of $\Exy[L(y, \pix)]$ unique over $\pi \in \Hspace$? Is the posterior distribution $p_{y|x}$ a resp. the minimizer of $\Exy[L(y, \pix)]$? Discuss the corresponding (dis-)advantanges of your findings.
\end{frame}

\begin{frame}{Solution to Question (d)}
	\begin{itemize}
		\item<1-> We can rewrite the 0-1-loss as $$L(y, \pix) = \id_{\pix \geq 1/2} \id_{y=0} + \id_{\pix <1/2} \id_{y=1}.$$
		\item<2-> Since $\Hspace = \{\pi: \Xspace \to [0, 1] \}$, we can optimize $\pi$ for each point $\xv$.
		\item<3-> In other words, for $\Exy[L(y, \pix)] = \Ex[\Eyx[L(y, \pix) \ | \ \xv]]$. we optimize $\Eyx[L(y, \pix) \ | \ \xv]$ for each $\xv$.
	\end{itemize}

\end{frame}

\begin{frame}{Solution to Question (d): Continued}
	\begin{align*}
		\Eyx[L(y, \pix) \ | \ \xv]
		&= \Eyx[\id_{\pix \geq 1/2} \id_{y=0} + \id_{\pix < 1/2} \id_{y=1} \ | \ \xv] \\
		\only<2->{& = \Eyx[\id_{\pix \geq 1/2} \id_{y=0} \ | \ \xv] + \Eyx[ \id_{\pix < 1/2} \id_{y=1} \ | \ \xv] \\}
		\only<3->{&= \id_{\pix \geq 1/2} \cdot \Eyx[\id_{y=0} \ | \ \xv] + \id_{\pix < 1/2} \cdot \Eyx[\id_{y=1} \ | \ \xv] \quad \rhd \\}
		\only<4->{&= \id_{\pix \geq 1/2} \Prob(y = 0 \ | \ \xv) + \id_{\pix < 1/2} \Prob(y = 1 | \xv). \\}
	\end{align*}
\end{frame}

\begin{frame}{Solution to Question (d): Continued}
	$$\Eyx[L(y, \pix) \ | \ \xv] = \id_{\pix \geq 1/2} \Prob(y = 0 \ | \ \xv) + \id_{\pix < 1/2} \Prob(y = 1 | \xv). $$
	We can distinguish between two cases:
	\begin{itemize}
		\item<2-> If $\Prob(y=0 \ | \ \xv) \geq 1/ 2$, then any $\pix < 1/2$ minimizes $\Eyx[L(y, \pix) \ | \ \xv]$.
		\item<3-> If $\Prob(y=0 \ | \ \xv) \leq 1/ 2$, then any $\pix \geq 1/2$ minimizes $\Eyx[L(y, \pix) \ | \ \xv]$.
	\end{itemize}
	
	\uncover<4->{
		In other words:
		$$
			\pix = \begin{cases}
				< 1/2, \quad &\text{if $\Prob(y = 0 \ | \ \xv) \geq 1 / 2$, } \\
				\geq 1/2, \quad &\text{if $\Prob(y=1 \ | \ \xv) < 1/2$.}
			\end{cases}
		$$
	}
	\uncover<5->{
		The posterior distribution $p_{y | \xv} ( 1 \ | \ \xv)$ is quite naturally of this form, but it is not the only $\pi$ of this kind. As a consequence, the minimize is not unique.
	}

\end{frame}

\begin{frame}{Solution to Question (d): Continued}
Disadvantages of using $p_{y | \xv}$:
\begin{itemize}
	\item TODO (The solution is not very clear. Ask people around.)
\end{itemize}
	
\end{frame}

\end{document}
