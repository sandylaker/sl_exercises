%% LaTeX Beamer presentation template (requires beamer package)
%% see http://bitbucket.org/rivanvx/beamer/wiki/Home
%% idea contributed by H. Turgut Uyar
%% template based on a template by Till Tantau
%% this template is still evolving - it might differ in future releases!

%% Template edited by Panagiotis Adamopoulos {padamopo}@stern.nyu.edu

\documentclass[aspectratio=169]{beamer}
 
\mode<presentation>
{
\usetheme{NYU}

\setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{comment}
%usepackage{appendixnumberbeamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfpages}
% citations
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\renewcommand{\bibsection}{\subsubsection*{\bibname } }

\input{latex-math/basic-math.tex}
\input{latex-math/basic-ml.tex}

\newcommand{\xik}{\xi_k}
\newcommand{\xij}{\xi_j}
\newcommand{\Imat}{\mathbf{I}}
\newcommand{\Vmat}{\mathbf{V}}
\newcommand{\Dmat}{\mathbf{D}}
\newcommand{\Pmat}{\mathbf{P}}
\newcommand{\Amattilde}{\tilde{\Amat}}



\title[]{\textbf{Exercise of Supervised Learning: \\ Regularization Part 2}}

%\subtitle{}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author{F.~Author\inst{1} \and S.~Another\inst{2}}
\author{Yawei Li} 

% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.
\institute[LMU]
{
\\
  \texttt{yawei.li@stat.uni-muenchen.de}
}

\date{December 10, 2024}


% This is only inserted into the PDF information catalog. Can be left
% out.
\subject{Subject}



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%\AtBeginSubsection[]
%{
%\begin{frame}<beamer>
%\frametitle{Outline}
%\tableofcontents[currentsection,currentsubsection]
%\end{frame}
%}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\begin{document}



\begin{frame}[noframenumbering, plain]
\titlepage

\end{frame}


\begin{frame}{Exercise 1}
	For a design matrix $\Xmat \in \mathbb{R}^{n \times p}$ and the vector of targets $\yv \in \mathbb{R}^n$, consider Lasso regression, i.e., 
	$$\argmin_{\thetab \in \mathbb{R}^p} 0.5 || \Xmat \thetab - \yv ||_2^2 + \lambda ||\thetab||_1. \qquad \lhd$$
	where $\lambda > 0$ is the regularization parameter.
	
	(a) Since there exists no analytical solution to Lasso regression in general, we want to find a procedure similar to gradient descent that should converge to the true solution.
	
	\vspace{10pt}
\end{frame}

\begin{frame}{Exercise 1 (a) (i)}
	\textbf{Question (a) (i)}: Explain why $\riskr$ is not differentiable.
	
	\vspace{10pt}
	\uncover<2->{
		\textbf{Solution}: Because $\lambda ||\thetab||_1$ is not differentiable at $\thetab = \zero$.
	}
\end{frame}

\begin{frame}{Exercise 1 (a) (ii)}
	\textbf{Question (a) (ii)}: Show that $\riskr$ is convex. \emph{Hint: The sum of convex function is convex.}
	
	\vspace{10pt} 
	
	\textbf{Solution}: 
		\begin{enumerate}
			\item<2-> $0.5 ||\Xmat \thetab - \yv ||_2^2$ is convex, since it is quadratic.
			\item<3-> $\lambda || \theta_1 ||_1$ is also convex (since it is a norm).
			\item<4-> Sum of convex functions is convex. So $\riskr$ is convex.
		\end{enumerate}
\end{frame}

\begin{frame}{Exercise 1 (a) (iii)}
	\small
	\textbf{Question (a) (iii)}: Find $\rho_j, z_j \in \mathbb{R}$ for which $$0.5 \frac{\partial}{\partial \theta_j} ||\Xmat \thetab - \yv ||_2^2 = -\rho_j + \theta_j z_j. \qquad \lhd$$
	\textbf{Solution}: 
	\begin{align*}
		\only<2->{
			\frac{\partial}{\partial \theta_j} ||\Xmat \thetab - \yv ||_2^2
		} 
		\only<3->{
			&= \frac{\partial}{\partial \theta_j} 0.5 \sumin \left(\yi - \sum_{k=1}^p \xik \theta_k \right)^2 \\
		}
		\only<4->{
			&= - \sumin \xij \left( \yi - \sum_{k=1}^p \xik \theta_k \right) \\
		}
		\only<5->{
			&= - \underbrace{\sumin \xij \left( \yi - \sum_{k\neq j}^p \xik \theta_k \right)}_{= \rho_j} + \theta_j \underbrace{\sumin \left( \xij \right)^2}_{ = z_j}
		}
	\end{align*}
\end{frame}

\begin{frame}[t]{Exercise 1(a) (iii)}
Show how to derive using the matrix form here.
	
\end{frame}

\begin{frame}{Exercise 1 (a) (iv)}
	\small
	\textbf{Question (a) (iv)}: In this situation, we can use the so-called sub-derivative which we denote with $\partial f$ for a real-valued convex continuous function $f$. The subderivative maps a point $\theta \in \mathbb{R}$ to and interval
	\begin{itemize}
		\item and if $f$ is differentiable at $\tilde{\theta} \in \mathbb{R}$, then $\partial f(\tilde{\theta}) = \{ \frac{\mathrm{d}}{\mathrm{d}\theta} f(\tilde{\theta}) \}$,
		\item and for $f(\theta) = \lambda |\theta|$ and $\lambda > 0$, it holds that $\partial f(\theta) = \begin{cases}
			\{ -\lambda \} \quad & \text{for } \theta < 0, \\
			[ -\lambda, \lambda] \quad & \text{for } \theta = 0, \\
			\{ \lambda \} \quad & \text{for } \theta > 0
		\end{cases}
		$
		\item and for $f, g$ real-valued convex functions with $\partial f(\tilde{\theta}) = [a, b], \partial g(\tilde{\theta}) = [c, d]$, $$\partial(f+g)(\tilde{\theta}) = [a + c, b + d]$$ where $b \geq a$ and $d \geq c$.
	\end{itemize}
	With this compute the sub-derivative of $\mathcal{R}_{\text{reg}, \thetab_{\neq j}}$ w.r.t. $\theta_j$, i.e., $\partial_{\theta_j} \mathcal{R}_{\text{reg}, \thetab_{\neq j}} $ where $\mathcal{R}_{\text{reg}, \thetab_{\neq j}}: \mathbb{R} \to \mathbb{R}_{\geq 0}, \theta_j \longmapsto \riskr(\theta_1, \ldots, \theta_j, \ldots, \theta_p)$ for constants $\thetab_{\neq j} = (\theta_1, \ldots, \theta_{j-1}, \theta_{j+1}, \ldots, \theta_p)^T$. \emph{Hint: Use (a) (iii).}
\end{frame}

\begin{frame}{Exercise 1 (a) (iv): Continued}
	\textbf{Solution}:
	Recall that $\riskr = 0.5 ||\Xmat \thetab - \yv ||_2^2 + \lambda ||\thetab||_1$ and $\frac{\partial}{\partial \theta_j} 0.5 ||\Xmat \thetab - \yv ||_2^2 = - \rho_j + \theta_j z_j$.
	Therefore,
	\uncover<2->{ 
		\begin{align*}
			\partial_{\theta_j} \mathcal{R}_{\text{reg}, \thetab_{\neq j}} &= \begin{cases}
					\{ -\rho_j + \theta_j z_j - \lambda \} \qquad & \text{for } \theta_j < 0, \\
					[-\rho_j -\lambda, -\rho_j + \lambda ] \qquad & \text{for } \theta_j = 0; \\
					\{- \rho_j + \theta_j z_j + \lambda  \} \qquad & \text{for } \theta_j > 0. 
			\end{cases}
		\end{align*}
	}
\end{frame}

\begin{frame}{Exercise 1 (a) (v)}

	\textbf{Question (a) (v)}: For a real-valued convex function $f$, the global minimum (if it exists) can be characterized in the following way:
		
		A point $\theta^* \in \mathbb{R}$ is the global minimum of $f$ if and only if $0 \in \partial f(\theta^*)$. 
		
		With this show that 
		$$\theta_j^* \in \argmin_{\theta_j \in \mathbb{R}} \mathcal{R}_{\text{reg}, \thetab_{\neq j}} \Leftrightarrow \theta_j^* = 
		\begin{cases}
			\frac{\rho_j + \lambda}{z_j} \qquad & \text{for } \rho_j < - \lambda \\
			0 \qquad & \text{for } - \lambda \leq \rho_j \leq \lambda \\
			\frac{\rho_j - \lambda}{z_j} \qquad & \text{for } \rho_j > \lambda  
		\end{cases}
		$$
\end{frame}

\begin{frame}{Exercise 1 (a) (v): Continued}
	\small
	\textbf{Solution}: we need to show that 
	\begin{align*}
		0 \in \partial_{\theta_j} \mathcal{R}_{\text{reg}, \thetab_{\neq j}} &= \begin{cases}
				\{ -\rho_j + \theta_j z_j - \lambda \} \qquad & \text{for } \theta_j < 0, \\
				[-\rho_j -\lambda, -\rho_j + \lambda ] \qquad & \text{for } \theta_j = 0; \\
				\{- \rho_j + \theta_j z_j + \lambda  \} \qquad & \text{for } \theta_j > 0. 
		\end{cases}
	\end{align*}
		
	where $z_j = \sumin \left( \xij \right)^2 \geq 0$.
	\begin{itemize}
		\item<2-> If $\theta_j < 0$, then $-\rho_j + \theta_j z_j - \lambda = 0$ leads to $\theta_j^* = \frac{\rho_j + \lambda}{z_j}$. Since $\theta_j^* <0$ and $z_j \geq 0$, we have $\rho_j < -\lambda$;
		\item<3-> If $\theta_j = 0$, then $\theta_j^* = 0$, and $-\rho_j - \lambda \leq 0 \leq -\rho_j + \lambda$ leads to $-\lambda \leq \rho_j \leq \lambda$, ;
		\item<4-> If $\theta_j > 0$ then $-\rho_j + \theta_j z_j + \lambda = 0$ leads to $\theta_j^* = \frac{\rho_j - \lambda}{z_j}$. Since $\theta_j^* > 0$, we have $\rho_j > \lambda$.
	\end{itemize}
	
\end{frame}

\begin{frame}{Exercise 1 (a) (v): Continued}
	Summarize everthing up:
	$$\theta_j^* \in \argmin_{\theta_j \in \mathbb{R}} \mathcal{R}_{\text{reg}, \thetab_{\neq j}} \Leftrightarrow \theta_j^* = 
		\begin{cases}
			\frac{\rho_j + \lambda}{z_j} \qquad & \text{for } \rho_j < - \lambda \\
			0 \qquad & \text{for } - \lambda \leq \rho_j \leq \lambda \\
			\frac{\rho_j - \lambda}{z_j} \qquad & \text{for } \rho_j > \lambda  
		\end{cases}
	$$
\end{frame}

\begin{frame}{Exercise 1 (a) (vi)}
	\textbf{Question (a) (vi)}: Plot $\theta_j^*$ as a function of $\rho_j$ for $\rho_j \in [-5, 5], \lambda =1, z_j = 1$. (This function is called soft thresholding operator).
	\vspace{10pt}
	
	\textbf{Solution}: \textbf{Show the standard solution.}
\end{frame}

\begin{frame}{Exercise 1 (b)}
	\small
	\textbf{Question (b)}: For non-singular $\Xmat^T \Xmat$, find the matrix $\Amat \in \mathbb{R}^{p \times p} $ for which $$\Amat^T \Xmat^T \Xmat \Amat = \Imat$$
	\emph{Hint: $\Xmat^T \Xmat$ is positive definite.}
	
	\textbf{Solution}: 
	\uncover<2->{
		Since $\Xmat^T \Xmat$ is positive definite, there exist an orthogonal matrix $\Vmat$ and a diagonal matrix $\Dmat$ with $D_{ii} > 0$ such that $$\Vmat \Dmat \Vmat^T = \Xmat^T \Xmat.$$
	}
	\uncover<3->{
		So 
		\begin{align*}
			\Amat^T \Xmat^T \Xmat \Amat = \Amat^T \Vmat \Dmat \Vmat^T \Amat = \Imat
		\end{align*}
	}
	\uncover<4->{
		To solve the equation for $\Amat$, we substitute the variable $ \Vmat^T \Amat = \Pmat$. Then, 
		\begin{align*}
			\Pmat^T \Dmat \Pmat = \Imat
		\end{align*}
	}
	\uncover<5->{
		Since $\Dmat$ is a diagnonal matrix, it can be seen that $\Pmat = \Dmat^{- 0.5}$. Hence $\Vmat^T \Amat = \Pmat$. Because $\Vmat$ is orthogonal, we have $\Amat = \Vmat \Dmat^{-0.5}$.
	}
\end{frame}

\begin{frame}{Exercise 1 (c) (i)}
	(c) For a design matrix with orthornormal columns, i.e., $\Xmat^T \Xmat = \Imat$, exists an analytical minimizer of the Lasso regression $\thetabh_{Lasso} = (\thetah_{Lasso, 1}, \ldots, \thetah_{Lasso, p})^T$ that is given by 
	$$\thetah_{Lasso, i} = \sign(\thetah_{i}) \max \left\{ \left|\thetah_i \right|  - \lambda, 0   \right\}, \quad i = i, \ldots, p,$$
	where $\thetabh = (\thetah_1, \ldots, \thetah_p)^T = (\Xmat^T \Xmat)^{-1} \Xmat^T \yv$ is the minimizer of the unregularized empirical risk (w.r.t. the L2 loss).
	
	Under the assumption that $\Xmat^T\Xmat$ is non-singular, your colleague proposes to project $\Xmat$ with $\Amat$ from (b), i.e., use $\Amattilde = \Xmat \Amat$ and then apply the analytical solution given here. 
	\vspace{10pt}
	
	\textbf{(i)} Show that this does not generally solve the original Lasso regression.
	 
	\vspace{10pt} 
	\emph{Hint: You only need to check under which condition $\nabla_{\thetab} 0.5 || \Xmat \thetab - \yv||_2^2 = \nabla_{\thetab} 0.5 ||\Xmat \Amat \thetab - \yv||_2^2 $. The proof can be finished with a subgradient argument regarding stationarity, which you do not have to do.}
\end{frame}

\begin{frame}{Exercise 1 (c) (i): Continued}
	\textbf{Solution}:
		\begin{align*}
			\nabla_{\thetab} 0.5 ||\Xmat \Amat \thetab - \yv||_2^2 &=  \Amat^T \Xmat^T \Xmat \Amat \thetab - \Amat^T \Xmat^T \yv \\
			\nabla_{\thetab} 0.5 ||\Xmat \thetab - \yv ||_2^2 &= \Xmat^T \Xmat \thetab - \Xmat^T \yv\\
		\end{align*}
		\uncover<2->{
			To let the gradient be equal, it must be satisfied that 
			$$\Amat = \Imat.$$
			In other words, if $\Amat \neq \Imat$, then the analytical solution does not solve the original Lasso regression.
		}
\end{frame}


\begin{frame}{Exercise 1 (c) (ii)}
	\textbf{Question (c) (ii)}: How could you adapt the penalty term such that the solution to the projected problem is equivalent to the original Lasso regression? In this case, can we still solve for parameters independently?
	
		\textbf{Solution:} 
		
		\begin{itemize}
			\item<2-> We can choose $||\Amat \thetab||_1$ as regularization term. This will yeild $\thetab^*_{\text{Projected}} = \Amat^{-1} \thetab_{\text{Lasso}}$. It is equivalent to the original Lasso regression in the sense that \textbf{the optimal solution leads to the same risk}. 
			\item<3-> However, due to the $||\Amat \thetab||_1$, it is (generally) not possible to split the risk into a sum of functiosn that depend on one parameter.
		\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (c) (iii)}
	\textbf{Question (c) (iii)}: Does the procedure proposed in \textbf{(c)} perform variable selection?
	
	\vspace{10pt}
	\textbf{Solution}: 
	\begin{itemize}
		\item<2-> We can see $||\Xmat \Amat \thetab -  \yv ||_2^2 + \lambda ||\thetab||_1$ as a projection of variables followed by a Lasso regression.
		\item<3-> Hence, we select variables in these projected coordinates. That is, for $(\Xmat \Amat) \thetab^* = \tilde{\Xmat} \thetab^*$, $\thetab^*$ select variables on $\tilde{\Xmat}$.
		\item<4-> But this does not imply that variable selection will occur on the original coordinates. To see this, $\Xmat \Amat \thetab^*$ needs to be viewed as $\Xmat (\Amat \thetab^*)$ as we now consider original features $\Xmat$.  In this case, $\Amat \thetab^*$ is in general non-sparse. Therefore, it does not perform variable selection on $\Xmat$.
	\end{itemize}
\end{frame}

\begin{frame}{Exercise 1 (d)}
	(d) You are given the following code to compare the quality of the projected Lasso regression vs. the regular Lasso regression. Complete the missing code of the algorithms and interpret the result.
	\vspace{20pt}
	
	\textbf{Solution: show the sandard solution.}
\end{frame}

\end{document}
